{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 64148,
          "databundleVersionId": 7669720,
          "sourceType": "competition"
        },
        {
          "sourceId": 726715,
          "sourceType": "datasetVersion",
          "datasetId": 262
        },
        {
          "sourceId": 7711309,
          "sourceType": "datasetVersion",
          "datasetId": 4484051
        },
        {
          "sourceId": 7909099,
          "sourceType": "datasetVersion",
          "datasetId": 4646195
        },
        {
          "sourceId": 7932508,
          "sourceType": "datasetVersion",
          "datasetId": 4662789
        },
        {
          "sourceId": 11371,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 5171
        }
      ],
      "dockerImageVersionId": 30648,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<!-- <center><img src=\"https://keras.io/img/logo-small.png\" alt=\"Keras logo\" width=\"100\"><br/>\n",
        "This starter notebook is provided by the Keras team.</center> -->"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "AJnEF8PaMj71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning Gemma for Answering Python Questions\n",
        "<!-- [KerasNLP](https://github.com/keras-team/keras-nlp) and [Keras](https://github.com/keras-team/keras) -->\n",
        "\n",
        "Notebook edited by: Kimaya Kulkarni\n",
        "\n",
        "<!-- > The objective of this competition is to build tools to assist Kaggle developers.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"https://i.ibb.co/8xZNc32/Gemma.png\">\n",
        "</div> -->\n",
        "<!--\n",
        "In this competition, we are asked to create notebooks that demonstrate how to use the Gemma LLM to accomplish one or more of the following developer-oriented tasks:\n",
        "1. Answer common questions about the Kaggle platform.\n",
        "2. Explain or teach basic data science concepts.\n",
        "3. Summarize Kaggle Solution write-ups.\n",
        "4. Explain or teach concepts from Kaggle Solution write-ups.\n",
        "5. **Answer common questions about the Python programming language.** -->\n",
        "\n",
        "<!-- This notebook guides you through performing `\"1. Answer common questions about the Kaggle platform\"` task for the competition. As this task requires specific knowledge of Kaggle, we need precise information about Kaggle. To do so, I have created a dataset, [\"Kaggle Docs\"](https://www.kaggle.com/datasets/awsaf49/kaggle-docs), collecting data from [kaggle.com/docs](https://www.kaggle.com/docs/). To make things easier for the model, the data is curated to have Question-Answer pair format, but if you are interested, the raw data is also available. We will use this dataset to fine-tune **Gemma LLM** to answer questions about the Kaggle platform. -->\n",
        "I have used the example notebook given and updated it to perform the task, Answering common questions about the Python Langauge. I have used a dataset available on kaggle (cleaned and reduced form) along with a dataset I made using various online resources including python documentation. Both datasets post cleaning consist of question - answer pairs used to tune the LLM.\n",
        "<!--\n",
        "<u>Fun fact</u>: This notebook is backend-agnostic, supporting TensorFlow, PyTorch, and JAX. However, the best performance can be achieved from `JAX`. Utilizing KerasNLP and Keras allows us to choose our preferred backend. Explore more details on [Keras](https://keras.io/keras_3/).\n",
        "\n",
        "**Note**: For a more in-depth understanding of KerasNLP, refer to the [KerasNLP guides](https://keras.io/keras_nlp/). -->\n"
      ],
      "metadata": {
        "id": "l8ObVuXZMj72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Libraries  "
      ],
      "metadata": {
        "id": "U9GLkZNLMj72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
        "# !pip install --upgrade \"packaging~=23.1\"\n",
        "!pip install -q -U keras-nlp\n",
        "!pip install -q -U keras>3"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-03-24T23:59:04.098913Z",
          "iopub.execute_input": "2024-03-24T23:59:04.099284Z",
          "iopub.status.idle": "2024-03-24T23:59:33.276855Z",
          "shell.execute_reply.started": "2024-03-24T23:59:04.099252Z",
          "shell.execute_reply": "2024-03-24T23:59:33.275861Z"
        },
        "trusted": true,
        "id": "U70-mxjJMj73",
        "outputId": "b1f4818f-e22b-462c-db9b-8f79c7c95bbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.1.1 which is incompatible.\ntensorflowjs 4.16.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "z06tvmoIMj73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\n",
        "\n",
        "import keras\n",
        "import keras_nlp\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas() # progress bar for pandas\n",
        "\n",
        "import plotly.graph_objs as go\n",
        "import plotly.express as px\n",
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-03-24T23:59:33.279107Z",
          "iopub.execute_input": "2024-03-24T23:59:33.279471Z",
          "iopub.status.idle": "2024-03-24T23:59:47.590862Z",
          "shell.execute_reply.started": "2024-03-24T23:59:33.279434Z",
          "shell.execute_reply": "2024-03-24T23:59:47.590007Z"
        },
        "trusted": true,
        "id": "1q2tyXE1Mj73",
        "outputId": "2e0e97a6-7583-432c-c2bc-72a476711f50"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-03-24 23:59:37.772641: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-24 23:59:37.772738: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-24 23:59:37.911295: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "AqBXpq-RMj73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    seed = 42\n",
        "    dataset_path = \"/kaggle/input/kaggle-docs/questions_answers\"\n",
        "    preset = \"gemma_2b_en\" # name of pretrained Gemma\n",
        "    sequence_length = 512 # max size of input sequence for training\n",
        "    batch_size = 1 # size of the input batch in training, x 2 as two GPUs\n",
        "    epochs = 5 # number of epochs to train"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-24T23:59:47.591949Z",
          "iopub.execute_input": "2024-03-24T23:59:47.592487Z",
          "iopub.status.idle": "2024-03-24T23:59:47.597257Z",
          "shell.execute_reply.started": "2024-03-24T23:59:47.592460Z",
          "shell.execute_reply": "2024-03-24T23:59:47.596415Z"
        },
        "trusted": true,
        "id": "2FSTm771Mj74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reproducibility\n",
        "Sets value for random seed to produce similar result in each run."
      ],
      "metadata": {
        "id": "oF8RYsxRMj74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.set_random_seed(CFG.seed)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-24T23:59:47.599183Z",
          "iopub.execute_input": "2024-03-24T23:59:47.599452Z",
          "iopub.status.idle": "2024-03-24T23:59:47.633852Z",
          "shell.execute_reply.started": "2024-03-24T23:59:47.599429Z",
          "shell.execute_reply": "2024-03-24T23:59:47.632996Z"
        },
        "trusted": true,
        "id": "rfaJYesjMj74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "\n",
        "The project's dataset was carefully crafted by combining self-generated FAQs from Python resources with an extensive Kaggle Stack Overflow dataset, creating a rich compilation tailored for fine-tuning the Gemma language model, focused on Python programming support and analysis.\n",
        "\n",
        "<!-- **Data Format:**\n",
        "\n",
        "=\n",
        "- This file includes:\n",
        "    - `Question`: A question about the Kaggle platform\n",
        "    - `Answer`: Answer to the question in markdown format\n",
        "    - `Category`: The category into which the question falls, one of the nine mentioned on the `kaggle.com/docs` website.\n",
        "    \n",
        "> You can access the **raw** data from `./kaggle-docs/raw/`, where there are `.txt` files for each of the **nine** categories. -->"
      ],
      "metadata": {
        "id": "svSFTSgvMj74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning Kaggle Dataset"
      ],
      "metadata": {
        "id": "G3cfT86vOF02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# each question in kaggle dataset is tagged by the topic, we only take those qns whose tag is python\n",
        "tags_df = pd.read_csv('/kaggle/input/pythonquestions/Tags.csv')\n",
        "python_tags = tags_df[tags_df['Tag'] == 'python']\n",
        "python_ids = python_tags['Id'].unique()\n",
        "#questions.csv is imported\n",
        "questions_df = pd.read_csv('/kaggle/input/pythonquestions/Questions.csv', encoding='latin1')\n",
        "#only tags which selected previously are kept\n",
        "questions_df = questions_df[questions_df['Id'].isin(python_ids)]\n",
        "#keep only columns we plan on using\n",
        "questions_df = questions_df[['Id','Score', 'Title', 'Body' ]]\n",
        "#clean data\n",
        "questions_df['Body'] = questions_df['Body'].str.replace('<.*?>', '', regex=True)\n",
        "#create new column for question\n",
        "questions_df['question'] = questions_df['Title'] + ' ' + questions_df['Body']\n",
        "\n",
        "#answers.csv\n",
        "answers_df = pd.read_csv('/kaggle/input/pythonquestions/Answers.csv', encoding='latin1')\n",
        "#only tags which selected previously are kept\n",
        "answers_df = answers_df[answers_df['ParentId'].isin(python_ids)]\n",
        "#max score ans is kept\n",
        "idx = answers_df.groupby('ParentId')['Score'].idxmax()\n",
        "answers_max_df = answers_df.loc[idx]\n",
        "answers_max_df = answers_max_df[['ParentId','Score', 'Body' ]]\n",
        "#clean text\n",
        "answers_max_df['Body'] = answers_max_df['Body'].str.replace('<.*?>', '', regex=True)\n",
        "\n",
        "#merge questions and answer dfs\n",
        "df1 = pd.merge(questions_df, answers_max_df, left_on='Id', right_on='ParentId')\n",
        "df1 = df1.sort_values(by='Score_y', ascending=False)\n",
        "df1 = df1.drop(columns=['ParentId','Score_x','Score_y','Title', 'Body_x', 'Id' ])\n",
        "df1.rename(columns={'Body_y': 'answer'}, inplace=True)\n",
        "print(df1.head())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-24T23:59:47.634981Z",
          "iopub.execute_input": "2024-03-24T23:59:47.635719Z",
          "iopub.status.idle": "2024-03-25T00:00:43.253067Z",
          "shell.execute_reply.started": "2024-03-24T23:59:47.635686Z",
          "shell.execute_reply": "2024-03-25T00:00:43.252125Z"
        },
        "trusted": true,
        "id": "UvnFOAf6Mj74",
        "outputId": "b8ae2740-ef02-4eed-91d7-6d4116183636"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "                                               question  \\\n962   What does the \"yield\" keyword do? What is the ...   \n334   What is a metaclass in Python? What are metacl...   \n4082  How to make a chain of function decorators in ...   \n1874  Does Python have a ternary conditional operato...   \n2662  Accessing the index in Python 'for' loops How ...   \n\n                                                 answer  \n962   To understand what yield does, you must unders...  \n334   Classes as objects\\n\\nBefore understanding met...  \n4082  If you are not into long explanations, see Pao...  \n1874  Yes, it was added in version 2.5.\\nThe syntax ...  \n2662  Using an additional state variable, such as an...  \n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# personal dataset built for the task\n",
        "df2 = pd.read_csv('/kaggle/input/python-q-1/python_coding_questions.csv')\n",
        "df2.head(2)\n",
        "print(df2.columns)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T00:00:43.254436Z",
          "iopub.execute_input": "2024-03-25T00:00:43.254824Z",
          "iopub.status.idle": "2024-03-25T00:00:43.266298Z",
          "shell.execute_reply.started": "2024-03-25T00:00:43.254791Z",
          "shell.execute_reply": "2024-03-25T00:00:43.265365Z"
        },
        "trusted": true,
        "id": "5_q3FeYEMj74",
        "outputId": "c2b7e2c8-4f87-469e-9e48-0491f5ca8b2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Index(['question', 'answer'], dtype='object')\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#combine the datasets, choose how many samples to use for training, here we take 500\n",
        "df_all = pd.concat([df2, df1], ignore_index=True)\n",
        "print(df_all.head())\n",
        "print(df_all.tail())\n",
        "df = df_all.iloc[:500]\n",
        "print(df.head())\n",
        "print(df.tail())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T00:00:43.267600Z",
          "iopub.execute_input": "2024-03-25T00:00:43.267863Z",
          "iopub.status.idle": "2024-03-25T00:00:43.302106Z",
          "shell.execute_reply.started": "2024-03-25T00:00:43.267840Z",
          "shell.execute_reply": "2024-03-25T00:00:43.301290Z"
        },
        "trusted": true,
        "id": "qTOhiFjXMj74",
        "outputId": "aca61c97-9cf0-4a81-ca22-141278c59fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "                                        question  \\\n0                How to create a list in Python?   \n1    How to iterate over a dictionary in Python?   \n2  How to check if a key exists in a dictionary?   \n3                 How to merge two dictionaries?   \n4                  How to sort a list in Python?   \n\n                                              answer  \n0    Use square brackets, e.g., my_list = [1, 2, 3].  \n1  Use .items() to iterate over key, value pairs,...  \n2              Use 'in', e.g., 'if key in my_dict:'.  \n3  Use the update() method or {**dict1, **dict2} ...  \n4       Use the sorted() function or .sort() method.  \n                                                 question  \\\n539333  First Python Code I installed the PyCharm and ...   \n539334  I am creating some voice authentication system...   \n539335  Using the return values of dynamic classes I h...   \n539336  How to flatten a nested \"homogeneous\" list I h...   \n539337  'NameError' in Python Program I'm working on a...   \n\n                                                   answer  \n539333  You use this:\\n\\nfor i in range(10):\\n   x = 0...  \n539334  Well\\n\\nFirst its an off topic question here.\\...  \n539335  When you iterate over the catalog, each time y...  \n539336  Here is my solution:\\n\\ndef homo_flat( seq ):\\...  \n539337  get rid of speedLimit = tryAndExceptInputInt(\"...  \n                                        question  \\\n0                How to create a list in Python?   \n1    How to iterate over a dictionary in Python?   \n2  How to check if a key exists in a dictionary?   \n3                 How to merge two dictionaries?   \n4                  How to sort a list in Python?   \n\n                                              answer  \n0    Use square brackets, e.g., my_list = [1, 2, 3].  \n1  Use .items() to iterate over key, value pairs,...  \n2              Use 'in', e.g., 'if key in my_dict:'.  \n3  Use the update() method or {**dict1, **dict2} ...  \n4       Use the sorted() function or .sort() method.  \n                                              question  \\\n495  How do you return multiple values in Python? T...   \n496  What is the easiest way to remove all packages...   \n497  Python: defaultdict of defaultdict? Is there a...   \n498  mysql_config not found when installing mysqldb...   \n499  \"Large data\" work flows using pandas I have tr...   \n\n                                                answer  \n495  Named tuples were added in 2.6 for this purpos...  \n496  I've found this snippet as an alternative solu...  \n497  Yes like this:\\n\\ndefaultdict(lambda : default...  \n498  mySQLdb is a python interface for mysql, but i...  \n499  I routinely use tens of gigabytes of data in j...  \n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the following simple template to create prompts from question-answer pairs and category to feed text into the model:\n",
        "\n",
        "```\n",
        "Category: ...\n",
        "\n",
        "Question: ...\n",
        "\n",
        "Answer: ...\n",
        "```\n",
        "\n",
        "This template helps the model understand what you're asking and how to respond accurately. You can explore more advanced prompt templates for better results."
      ],
      "metadata": {
        "id": "gB__TJ15Mj74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt template\n",
        "template = \"\\n\\nQuestion:\\n{question}\\n\\nAnswer:\\n{answer}\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T00:00:43.303041Z",
          "iopub.execute_input": "2024-03-25T00:00:43.303302Z",
          "iopub.status.idle": "2024-03-25T00:00:43.307177Z",
          "shell.execute_reply.started": "2024-03-25T00:00:43.303279Z",
          "shell.execute_reply": "2024-03-25T00:00:43.306337Z"
        },
        "trusted": true,
        "id": "elLFmsfuMj75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "template = \"\\n\\nQuestion:\\n{question}\\n\\nAnswer:\\n{answer}\"\n",
        "\n",
        "df[\"prompt\"] = df.progress_apply(lambda row: template.format(\n",
        "                                                    question=row['question'],\n",
        "                                                    answer=row['answer']), axis=1)\n",
        "data = df['prompt'].tolist()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T00:00:43.308225Z",
          "iopub.execute_input": "2024-03-25T00:00:43.308481Z",
          "iopub.status.idle": "2024-03-25T00:00:43.353975Z",
          "shell.execute_reply.started": "2024-03-25T00:00:43.308450Z",
          "shell.execute_reply": "2024-03-25T00:00:43.353110Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "3ff4eebb2d8746079d82b90d3fb6a2bd"
          ]
        },
        "id": "EPZa0YkIMj75",
        "outputId": "8d556b67-e607-488b-877d-7a680bbbd4b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/500 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ff4eebb2d8746079d82b90d3fb6a2bd"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_35/3894174901.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df[\"prompt\"] = df.progress_apply(lambda row: template.format(\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's examine a sample prompt. As the answers in our dataset are curated with **markdown** format, we will render the sample using `Markdown()` to properly visualize the formatting."
      ],
      "metadata": {
        "id": "S5cGJb6jMj75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample"
      ],
      "metadata": {
        "id": "jHF9ZkmUMj75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def colorize_text(text):\n",
        "    for word, color in zip([ \"question\", \"answer\"], [ \"red\", \"green\"]):\n",
        "        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
        "    return text"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-03-25T00:00:43.362433Z",
          "iopub.execute_input": "2024-03-25T00:00:43.362689Z",
          "iopub.status.idle": "2024-03-25T00:00:43.372424Z",
          "shell.execute_reply.started": "2024-03-25T00:00:43.362668Z",
          "shell.execute_reply": "2024-03-25T00:00:43.371397Z"
        },
        "trusted": true,
        "id": "49gfZCH4Mj75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a random sample\n",
        "sample = data[45]\n",
        "\n",
        "# Give colors to Question, Answer and Category\n",
        "sample = colorize_text(sample)\n",
        "\n",
        "# Show sample in markdown\n",
        "display(Markdown(sample))"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-03-25T00:00:43.373540Z",
          "iopub.execute_input": "2024-03-25T00:00:43.373820Z",
          "iopub.status.idle": "2024-03-25T00:00:43.385767Z",
          "shell.execute_reply.started": "2024-03-25T00:00:43.373794Z",
          "shell.execute_reply": "2024-03-25T00:00:43.384983Z"
        },
        "trusted": true,
        "id": "48MD4jtVMj75",
        "outputId": "e61dc729-02c9-44ca-815f-c0a25f1aa2c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "\n\nQuestion:\nHow to reverse a list?\n\nAnswer:\nUse the reverse() method or slicing [::-1]."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M-ArnuvcMj75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis\n",
        "\n",
        "Let's do a simple EDA to determine how many question-answer pairs we have per category."
      ],
      "metadata": {
        "id": "ivr6R5ulMj75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot question length vs count distribution\n",
        "df_copy = df.copy()\n",
        "# Example assuming you want to analyze the length of questions\n",
        "df_copy['QuestionLength'] = df_copy['question'].apply(len)\n",
        "\n",
        "# Get unique lengths and their frequency\n",
        "unique_lengths, length_counts = np.unique(df_copy['QuestionLength'], return_counts=True)\n",
        "\n",
        "print(unique_lengths)\n",
        "print(length_counts)\n",
        "plt.plot(unique_lengths, length_counts)\n",
        "plt.title(\"Question Length Distribution\")\n",
        "plt.xlabel(\"Question Length\")\n",
        "plt.ylabel(\"Count\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T00:00:43.396424Z",
          "iopub.execute_input": "2024-03-25T00:00:43.396695Z",
          "iopub.status.idle": "2024-03-25T00:00:43.676672Z",
          "shell.execute_reply.started": "2024-03-25T00:00:43.396674Z",
          "shell.execute_reply": "2024-03-25T00:00:43.675730Z"
        },
        "trusted": true,
        "id": "1AQ0ZxtSMj75",
        "outputId": "ae593b3f-068f-4548-c6f1-719fdd60f646"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[   22    29    30    31    33    37    43    45    53    63    68    79\n    86    88    91    98    99   101   102   104   105   107   109   111\n   113   114   118   121   122   123   125   126   127   129   130   131\n   137   142   143   144   145   147   148   149   150   152   153   154\n   155   156   157   158   159   161   162   165   167   169   173   174\n   175   178   179   180   181   183   184   185   186   187   188   189\n   190   191   192   196   199   200   202   203   204   205   206   209\n   211   212   213   216   217   218   219   223   225   227   229   231\n   233   234   235   244   245   246   247   248   250   251   254   256\n   257   258   259   260   262   263   264   266   268   269   272   274\n   280   283   284   285   287   291   292   293   295   298   303   304\n   305   311   316   318   319   320   322   323   328   330   332   333\n   334   336   338   341   347   349   352   353   354   357   360   362\n   369   370   372   376   378   379   382   383   384   389   392   394\n   395   399   401   404   405   406   407   409   410   411   412   413\n   414   416   422   423   424   427   429   431   433   434   439   440\n   441   443   446   452   453   454   455   457   458   459   462   465\n   466   467   470   473   477   479   483   484   488   490   491   494\n   495   497   501   505   506   511   518   531   533   548   553   555\n   557   558   562   565   572   574   581   585   600   603   605   609\n   611   614   619   626   632   635   637   640   641   645   651   657\n   659   662   668   674   676   679   683   691   693   697   701   703\n   707   717   718   728   733   742   752   754   762   791   796   801\n   816   828   844   846   860   883   911   932   935   941   966   994\n  1010  1039  1050  1072  1075  1084  1093  1139  1149  1162  1176  1178\n  1186  1245  1275  1305  1308  1369  1399  1402  1484  1517  1593  1623\n  1803  1831  1967  1981  2045  2077  2183  2245  2326  2779  2823  2999\n  3110  3855  4540  4972  6855  9610 12461]\n[10 20 10 10 10 10 10 10 10  1  1  2  1  1  1  1  3  2  1  1  1  1  1  2\n  1  1  1  2  3  2  1  1  2  2  1  2  2  4  1  1  1  1  1  1  2  2  1  1\n  1  1  2  1  1  1  1  1  1  1  1  1  2  3  1  1  1  1  1  1  3  1  4  1\n  1  1  1  1  2  1  3  1  1  2  1  2  2  1  2  2  1  1  1  1  2  1  1  3\n  1  1  2  2  1  1  1  1  2  1  2  1  1  1  2  1  1  1  1  1  1  1  1  1\n  1  1  2  1  1  2  1  3  1  1  3  1  1  1  3  1  1  2  1  2  2  1  1  1\n  1  3  2  1  1  1  1  1  2  1  1  2  1  2  3  1  1  1  1  1  2  1  1  1\n  1  2  1  1  2  1  1  1  2  1  1  1  1  1  1  1  2  1  1  1  1  2  1  2\n  1  1  1  3  1  1  1  1  1  1  1  1  1  2  1  1  1  1  2  1  2  1  1  1\n  1  1  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1\n  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n  1  1  1  1  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1  1\n  1  1  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1]\n",
          "output_type": "stream"
        },
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Text(0, 0.5, 'Count')"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 640x480 with 1 Axes>",
            "image/png": "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"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling\n",
        "\n",
        "<!-- <div align=\"center\"><img src=\"https://i.ibb.co/Bqg9w3g/Gemma-Logo-no-background.png\" width=\"300\"></div> -->\n",
        "\n",
        "**Gemma** is a suite of advanced open models developed by **Google DeepMind** and other **Google teams**, derived from the same research and technology behind the **Gemini** models. They can be integrated into applications and run on various platforms including mobile devices and hosted services. Developers can customize Gemma models using tuning techniques to enhance their performance for specific tasks, offering more targeted and efficient generative AI solutions beyond text generation.\n",
        "\n",
        "Gemma models are available in several sizes so you can build generative AI solutions based on your available computing resources, the capabilities you need, and where you want to run them.\n",
        "\n",
        "| Parameters size | Tuned versions    | Intended platforms                 | Preset                 |\n",
        "|-----------------|-------------------|------------------------------------|------------------------|\n",
        "| 2B              | Pretrained        | Mobile devices and laptops         | `gemma_2b_en`          |\n",
        "| 2B              | Instruction tuned | Mobile devices and laptops         | `gemma_instruct_2b_en` |\n",
        "| 7B              | Pretrained        | Desktop computers and small servers| `gemma_7b_en`          |\n",
        "| 7B              | Instruction tuned | Desktop computers and small servers| `gemma_instruct_7b_en` |\n",
        "\n",
        "In this notebook, we will use the `Gemma 2B` from KerasNLP's pretrained models to answer questions about python programming.\n",
        "\n",
        "<!-- To explore other models, simply modify the `preset` in the `CFG` (config). A list of other available pretrained models can be found on the [KerasNLP website](https://keras.io/api/keras_nlp/models/). -->\n",
        "\n"
      ],
      "metadata": {
        "id": "Hi58j9owMj75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemma Causal LM\n",
        "\n",
        "The code below will build an end-to-end Gemma model for causal language modeling (hence the name `GemmaCausalLM`). A causal language model (LM) predicts the next token based on previous tokens. This task setup can be used to train the model unsupervised on plain text input or to autoregressively generate plain text similar to the data used for training. This task can be used for pre-training or fine-tuning a Gemma model simply by calling `fit()`.\n",
        "\n",
        "This model has a `generate()` method, which generates text based on a prompt. The generation strategy used is controlled by an additional sampler argument on `compile()`. You can recompile the model with different `keras_nlp.samplers` objects to control the generation. By default, `\"greedy\"` sampling will be used.\n",
        "\n",
        "> The `from_preset` method instantiates the model from a preset architecture and weights."
      ],
      "metadata": {
        "id": "n5dJ60MdMj75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n",
        "gemma_lm.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T00:00:43.677650Z",
          "iopub.execute_input": "2024-03-25T00:00:43.677898Z",
          "iopub.status.idle": "2024-03-25T00:01:42.203879Z",
          "shell.execute_reply.started": "2024-03-25T00:00:43.677876Z",
          "shell.execute_reply": "2024-03-25T00:01:42.202988Z"
        },
        "trusted": true,
        "id": "k7_y9AxnMj75",
        "outputId": "4cbe9b09-9817-4031-bcf9-3712c1f47b10"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/2' to your Kaggle notebook...\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\n\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m\n\n gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                                                                \u001b[38;5;34m256,000\u001b[0m \n\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span><span style=\"font-weight: bold\">                                             Vocab # </span>\n\n gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                                                                <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> \n\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\n\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m\n\n padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                             \u001b[38;5;34m0\u001b[0m  -                          \n\n token_ids (\u001b[38;5;33mInputLayer\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                             \u001b[38;5;34m0\u001b[0m  -                          \n\n gemma_backbone                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           \u001b[38;5;34m2,506,172,416\u001b[0m  padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        \n (\u001b[38;5;33mGemmaBackbone\u001b[0m)                                                            token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n\n token_embedding                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)           \u001b[38;5;34m524,288,000\u001b[0m  gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mReversibleEmbedding\u001b[0m)                                                                                 \n\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n<span style=\"font-weight: bold\"> Layer (type)                  </span><span style=\"font-weight: bold\"> Output Shape              </span><span style=\"font-weight: bold\">         Param # </span><span style=\"font-weight: bold\"> Connected to               </span>\n\n padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                          \n\n token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                          \n\n gemma_backbone                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span>  padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)                                                            token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n\n token_embedding                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span>  gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)                                                                                 \n\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemma LM Preprocessor\n",
        "\n",
        "An important part of the Gemma model is the **Preprocessor** layer, which under the hood uses **Tokenizer**.\n",
        "\n",
        "**What it does:** The preprocessor takes input strings and transforms them into a dictionary (`token_ids`, `padding_mask`) containing preprocessed tensors. This process starts with tokenization, where input strings are converted into sequences of token IDs.\n",
        "\n",
        "**Why it's important:** Initially, raw text data is complex and challenging for modeling due to its high dimensionality. By converting text into a compact set of tokens, such as transforming `\"The quick brown fox\"` into `[\"the\", \"qu\", \"##ick\", \"br\", \"##own\", \"fox\"]`, we simplify the data. Many models rely on special tokens and additional tensors to understand input. These tokens help divide input and identify padding, among other tasks. Making all sequences the same length through padding boosts computational efficiency, making subsequent steps smoother.\n",
        "\n",
        "Explore the following pages to access the available preprocessing and tokenizer layers in **KerasNLP**:\n",
        "- [Preprocessing](https://keras.io/api/keras_nlp/preprocessing_layers/)\n",
        "- [Tokenizers](https://keras.io/api/keras_nlp/tokenizers/)"
      ],
      "metadata": {
        "id": "GWcpii0SMj75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y, sample_weight = gemma_lm.preprocessor(data[0:2])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T00:01:42.205095Z",
          "iopub.execute_input": "2024-03-25T00:01:42.206874Z",
          "iopub.status.idle": "2024-03-25T00:01:42.534238Z",
          "shell.execute_reply.started": "2024-03-25T00:01:42.206847Z",
          "shell.execute_reply": "2024-03-25T00:01:42.533456Z"
        },
        "trusted": true,
        "id": "AB2_O7kTMj75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This preprocessing layer will take in batches of strings, and return outputs in a `(x, y, sample_weight)` format, where the `y` label is the next token id in the `x` sequence.\n",
        "\n",
        "From the code below, we can see that, after the preprocessor, the data shape is `(num_samples, sequence_length)`."
      ],
      "metadata": {
        "id": "KGd21VcJMj75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the shape of each processed output\n",
        "for k, v in x.items():\n",
        "    print(k, \":\", v.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T00:01:42.535542Z",
          "iopub.execute_input": "2024-03-25T00:01:42.535835Z",
          "iopub.status.idle": "2024-03-25T00:01:42.540944Z",
          "shell.execute_reply.started": "2024-03-25T00:01:42.535810Z",
          "shell.execute_reply": "2024-03-25T00:01:42.540067Z"
        },
        "trusted": true,
        "id": "gyVdiwhVMj76",
        "outputId": "dcc7af19-9914-4756-efc2-dcac838215b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "token_ids : (2, 8192)\npadding_mask : (2, 8192)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference before fine tuning\n",
        "\n",
        "Let's ask the Gemma model some sample questions using our prepared prompt and see how it responds.\n",
        "\n",
        "> Our initial tests with the Gemma model on Python queries revealed a need for fine-tuning, as responses lacked precision and contextual relevance. Enhancing the model with a targeted dataset will improve its accuracy and utility in addressing programming-specific questions."
      ],
      "metadata": {
        "id": "rLvRWu62Mj76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample 1"
      ],
      "metadata": {
        "id": "9xM5W7i6Mj76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take one sample\n",
        "row = df.iloc[2]\n",
        "\n",
        "# Generate Prompt using template\n",
        "prompt = template.format(\n",
        "\n",
        "    question=row.question,\n",
        "    answer=\"\"\n",
        ")\n",
        "\n",
        "# Infer\n",
        "output = gemma_lm.generate(prompt, max_length=256)\n",
        "\n",
        "# Colorize\n",
        "output = colorize_text(output)\n",
        "\n",
        "# Display in markdown\n",
        "display(Markdown(output))"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-03-25T00:01:42.542229Z",
          "iopub.execute_input": "2024-03-25T00:01:42.542581Z",
          "iopub.status.idle": "2024-03-25T00:01:59.139294Z",
          "shell.execute_reply.started": "2024-03-25T00:01:42.542550Z",
          "shell.execute_reply": "2024-03-25T00:01:59.138268Z"
        },
        "trusted": true,
        "id": "roWDvS2HMj76",
        "outputId": "8ca26645-8682-4dfb-e26f-bc58394fef21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "\n\nQuestion:\nHow to check if a key exists in a dictionary?\n\nAnswer:\nThe in operator can be used to check if a key exists in a dictionary.\n\nExample:\n\n>>> d = {'a': 1, 'b': 2, 'c': 3}\n>>> d.get('a')\n1\n>>> d.get('d')\n>>> d.get('e')\n>>> d.get('f')\n>>> d.get('g')\n>>> d.get('h')\n>>> d.get('i')\n>>> d.get('j')\n>>> d.get('k')\n>>> d.get('l')\n>>> d.get('m')\n>>> d.get('n')\n>>> d.get('o')\n>>> d.get('p')\n>>> d.get('q')\n>>> d.get('r')\n>>> d.get('s')\n>>> d.get('t')\n>>> d.get('u')\n>>> d.get('v')\n>>> d.get('w')\n>>> d.get('x')\n>>> d.get('y')\n>>> d.get('z')\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample 2"
      ],
      "metadata": {
        "id": "nLPP4R8iMj76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take one sample\n",
        "row = df.iloc[250]\n",
        "\n",
        "# Generate Prompt using template\n",
        "prompt = template.format(\n",
        "\n",
        "    question=row.question,\n",
        "    answer=\"\"\n",
        ")\n",
        "\n",
        "# Infer\n",
        "output = gemma_lm.generate(prompt, max_length=256)\n",
        "\n",
        "# Colorize\n",
        "output = colorize_text(output)\n",
        "\n",
        "# Display in markdown\n",
        "display(Markdown(output))"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-03-25T00:01:59.140694Z",
          "iopub.execute_input": "2024-03-25T00:01:59.141350Z",
          "iopub.status.idle": "2024-03-25T00:02:02.139545Z",
          "shell.execute_reply.started": "2024-03-25T00:01:59.141313Z",
          "shell.execute_reply": "2024-03-25T00:02:02.138546Z"
        },
        "trusted": true,
        "id": "rUSDKJrqMj76",
        "outputId": "b2ffb8b3-d5d5-4ace-bac9-8d479e6e86d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "\n\nQuestion:\nPython `if x is not None` or `if not x is None`? I've always thought of the if not x is None version to be more clear, but Google's style guide implies (based on this excerpt) that they use if x is not None. Is there any minor performance difference (I'm assuming not), and is there any case where one really doesn't fit (making the other a clear winner for my convention)?*\n\n*I'm referring to any singleton, rather than just None.\n\n\n  ...to compare singletons like\n  None. Use is  or is not.\n\n\n\nAnswer:\nThe difference is that the first one is a comparison of the object itself, while the second one is a comparison of the object's identity.\n\nThe first one is more efficient, because it doesn't have to check the object's identity.\n\nThe second one is more efficient, because it doesn't have to check the object's identity.\n\nThe second one is more efficient, because it doesn't have to check the object's identity.\n\nThe second one is more efficient, because it doesn't have to check the object's identity.\n\nThe"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning with LoRA\n",
        "\n",
        "To get better responses from the model, we will fine-tune the model with Low Rank Adaptation (LoRA) on the **Kaggle Docs** dataset.\n",
        "\n",
        "**What exactly is LoRA?**\n",
        "\n",
        "LoRA is a method used to fine-tune large language models (LLMs) in an efficient way. It involves freezing the weights of the LLM and injecting trainable rank-decomposition matrices.\n",
        "\n",
        "Imagine in an LLM, we have a pre-trained dense layer, represented by a $d \\times d$ weight matrix, denoted as $W_0$. We then initialize two additional dense layers, labeled as $A$ and $B$, with shapes $d \\times r$ and $r \\times d$, respectively. Here, $r$ denotes the rank, which is typically **much smaller than** $d$. Prior to LoRA, the model's output was computed using the equation $output = W_0 \\cdot x + b_0$, where $x$ represents the input and $b_0$ denotes the bias term associated with the original dense layer, which remains frozen. After applying LoRA, the equation becomes $output = (W_0 \\cdot x + b_0) + (B \\cdot A \\cdot x)$, where $A$ and $B$ denote the trainable rank-decomposition matrices that have been introduced.\n",
        "\n",
        "<center><img src=\"https://i.ibb.co/DWsbhLg/LoRA.png\" width=\"300\"><br/>\n",
        "Credit: <a href=\"https://arxiv.org/abs/2106.09685\">LoRA: Low-Rank Adaptation of Large Language Models</a> Paper</center>\n",
        "\n",
        "\n",
        "In the paper, $A$ is initialized with $\\mathcal{N} (0, \\sigma^2)$ and $B$ with $0$, where $\\mathcal{N}$ denotes the normal distribution, and $\\sigma^2$ is the variance.\n",
        "\n",
        "**Why does LoRA save memory?**\n",
        "\n",
        "Even though we're adding more layers to the model with LoRA, it actually helps save memory. This is because the smaller layers (A and B) have fewer parameters to learn compared to the big model and fewer trainable parameters mean fewer optimizer variables to store. So, even though the overall model might seem bigger, it's actually more efficient in terms of memory usage.\n",
        "\n",
        "> This notebook uses a LoRA rank of `4`. A higher rank means more detailed changes are possible, but also means more trainable parameters."
      ],
      "metadata": {
        "id": "_3VujTuUMj76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable LoRA for the model and set the LoRA rank to 4.\n",
        "gemma_lm.backbone.enable_lora(rank=4)\n",
        "gemma_lm.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T00:02:02.140668Z",
          "iopub.execute_input": "2024-03-25T00:02:02.140958Z",
          "iopub.status.idle": "2024-03-25T00:02:02.594418Z",
          "shell.execute_reply.started": "2024-03-25T00:02:02.140932Z",
          "shell.execute_reply": "2024-03-25T00:02:02.593544Z"
        },
        "trusted": true,
        "id": "K5dwFaCFMj79",
        "outputId": "3c22dcaa-f833-4c6a-afdd-10f5ae473cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\n\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m\n\n gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                                                                \u001b[38;5;34m256,000\u001b[0m \n\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span><span style=\"font-weight: bold\">                                             Vocab # </span>\n\n gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                                                                <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> \n\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\n\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m\n\n padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                             \u001b[38;5;34m0\u001b[0m  -                          \n\n token_ids (\u001b[38;5;33mInputLayer\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                             \u001b[38;5;34m0\u001b[0m  -                          \n\n gemma_backbone                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           \u001b[38;5;34m2,507,536,384\u001b[0m  padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        \n (\u001b[38;5;33mGemmaBackbone\u001b[0m)                                                            token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n\n token_embedding                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)           \u001b[38;5;34m524,288,000\u001b[0m  gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n (\u001b[38;5;33mReversibleEmbedding\u001b[0m)                                                                                 \n\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n<span style=\"font-weight: bold\"> Layer (type)                  </span><span style=\"font-weight: bold\"> Output Shape              </span><span style=\"font-weight: bold\">         Param # </span><span style=\"font-weight: bold\"> Connected to               </span>\n\n padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                          \n\n token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                          \n\n gemma_backbone                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span>  padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)                                                            token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n\n token_embedding                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span>  gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)                                                                                 \n\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,507,536,384\u001b[0m (9.34 GB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> (9.34 GB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,363,968\u001b[0m (5.20 MB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,363,968</span> (5.20 MB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notice** that, the number of trainable parameters is reduced from ~$2.5$ billions to ~$1.3$ millions after enabling LoRA."
      ],
      "metadata": {
        "id": "x0OouIP5Mj79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use AdamW (a common optimizer for transformer models).\n",
        "optimizer = keras.optimizers.AdamW(\n",
        "    learning_rate=5e-6,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T00:02:02.595612Z",
          "iopub.execute_input": "2024-03-25T00:02:02.595898Z",
          "iopub.status.idle": "2024-03-25T00:02:02.601596Z",
          "shell.execute_reply.started": "2024-03-25T00:02:02.595873Z",
          "shell.execute_reply": "2024-03-25T00:02:02.600563Z"
        },
        "trusted": true,
        "id": "bF1hMY6jMj79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "ulZaJZTLMj79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Limit the input sequence length to 512 (to control memory usage).\n",
        "gemma_lm.preprocessor.sequence_length = CFG.sequence_length\n",
        "\n",
        "# Compile the model with loss, optimizer, and metric\n",
        "gemma_lm.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=optimizer,\n",
        "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Train model\n",
        "gemma_lm.fit(data, epochs=5, batch_size=CFG.batch_size)\n",
        "# gemma_lm.fit(data, epochs=CFG.epochs, batch_size=CFG.batch_size)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-25T00:02:02.602634Z",
          "iopub.execute_input": "2024-03-25T00:02:02.602901Z",
          "iopub.status.idle": "2024-03-25T00:32:44.964476Z",
          "shell.execute_reply.started": "2024-03-25T00:02:02.602878Z",
          "shell.execute_reply": "2024-03-25T00:32:44.963566Z"
        },
        "trusted": true,
        "id": "orcfGFNtMj79",
        "outputId": "d5ef4025-3bd1-4b99-fcb2-3b215ff2413a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/5\n\u001b[1m500/500\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m385s\u001b[0m 728ms/step - loss: 0.5898 - sparse_categorical_accuracy: 0.5749\nEpoch 2/5\n\u001b[1m500/500\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m364s\u001b[0m 727ms/step - loss: 0.5819 - sparse_categorical_accuracy: 0.5776\nEpoch 3/5\n\u001b[1m500/500\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m364s\u001b[0m 727ms/step - loss: 0.5729 - sparse_categorical_accuracy: 0.5808\nEpoch 4/5\n\u001b[1m500/500\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m364s\u001b[0m 727ms/step - loss: 0.5624 - sparse_categorical_accuracy: 0.6015\nEpoch 5/5\n\u001b[1m500/500\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m364s\u001b[0m 727ms/step - loss: 0.5537 - sparse_categorical_accuracy: 0.6125\n",
          "output_type": "stream"
        },
        {
          "execution_count": 22,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<keras.src.callbacks.history.History at 0x7c463c484670>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference after fine-tuning\n",
        "\n",
        "Let's see how our fine-tuned model responds to the same questions we asked before fine-tuning the model."
      ],
      "metadata": {
        "id": "6EYu6Si4Mj79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample 1"
      ],
      "metadata": {
        "id": "Nx3xIZKuMj79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take one sample\n",
        "row = df.iloc[2]\n",
        "\n",
        "# Generate Prompt using template\n",
        "prompt = template.format(\n",
        "\n",
        "    question=row.question,\n",
        "    answer=\"\"\n",
        ")\n",
        "\n",
        "# Infer\n",
        "output = gemma_lm.generate(prompt, max_length=256)\n",
        "\n",
        "# Colorize\n",
        "output = colorize_text(output)\n",
        "\n",
        "# Display in markdown\n",
        "display(Markdown(output))"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-03-25T00:32:44.965756Z",
          "iopub.execute_input": "2024-03-25T00:32:44.966095Z",
          "iopub.status.idle": "2024-03-25T00:33:01.272202Z",
          "shell.execute_reply.started": "2024-03-25T00:32:44.966068Z",
          "shell.execute_reply": "2024-03-25T00:33:01.271229Z"
        },
        "trusted": true,
        "id": "8LRylUdzMj79",
        "outputId": "0b893ab5-f2de-4c66-e4f5-cdc6282278cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "\n\nQuestion:\nHow to check if a key exists in a dictionary?\n\nAnswer:\nYou can use either the .has_key() or the .get() methods to check if a key exists in a dictionary:\n\n>>> d = {\"a\": 1, \"b\": 2}\n>>> d.has_key(\"a\") # Returns True\nTrue\n>>> d.has_key(\"c\") # Returns False\nFalse\n>>> d.get(\"a\") # If key doesn't exist, this will return a default value\n>>> d.get(\"c\") # If key doesn't exist, this will raise KeyError exception\n\n\nNote that if a key doesn't exist in a dictionary, the get() method will return the default (if specified) for the key. If the default is not specified, the get() method will raise a KeyError exception.\n\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample 2"
      ],
      "metadata": {
        "id": "Q2cGmC3DMj79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take one sample\n",
        "row = df.iloc[250]\n",
        "\n",
        "# Generate Prompt using template\n",
        "prompt = template.format(\n",
        "\n",
        "    question=row.question,\n",
        "    answer=\"\"\n",
        ")\n",
        "\n",
        "# Infer\n",
        "output = gemma_lm.generate(prompt, max_length=256)\n",
        "\n",
        "# Colorize\n",
        "output = colorize_text(output)\n",
        "\n",
        "# Display in markdown\n",
        "display(Markdown(output))"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-03-25T00:33:01.273551Z",
          "iopub.execute_input": "2024-03-25T00:33:01.273881Z",
          "iopub.status.idle": "2024-03-25T00:33:04.705543Z",
          "shell.execute_reply.started": "2024-03-25T00:33:01.273854Z",
          "shell.execute_reply": "2024-03-25T00:33:04.704451Z"
        },
        "trusted": true,
        "id": "grl4mVI6Mj79",
        "outputId": "4f4754c0-8d02-4ee7-f0a7-a41648d34b1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "\n\nQuestion:\nPython `if x is not None` or `if not x is None`? I've always thought of the if not x is None version to be more clear, but Google's style guide implies (based on this excerpt) that they use if x is not None. Is there any minor performance difference (I'm assuming not), and is there any case where one really doesn't fit (making the other a clear winner for my convention)?*\n\n*I'm referring to any singleton, rather than just None.\n\n\n  ...to compare singletons like\n  None. Use is  or is not.\n\n\n\nAnswer:\nThe difference is only in speed, and only in Python.\n\nIf you're using an object, and you want to test if the reference to that object is None, then the first form of your if is faster than the second.\n\n>>> import time\n>>> x = object()\n>>> t= time.time()\n>>> x is None\n>>> time.time() - t\n0.000020\n>>> t = time.time()\n>>> x is not None\n>>> time.time() - t\n0.000023"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unseen Sample\n",
        "\n",
        "Also just for fun, let's try out a question that model hasn't seen during training."
      ],
      "metadata": {
        "id": "AcXMLu1KMj7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Prompt using template\n",
        "prompt = template.format(\n",
        "\n",
        "    question=\"How to export a notebook?\",\n",
        "    answer=\"\"\n",
        ")\n",
        "\n",
        "# Infer\n",
        "output = gemma_lm.generate(prompt, max_length=256)\n",
        "\n",
        "# Colorize\n",
        "output = colorize_text(output)\n",
        "\n",
        "# Display in markdown\n",
        "display(Markdown(output))"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-03-25T00:33:04.706903Z",
          "iopub.execute_input": "2024-03-25T00:33:04.707323Z",
          "iopub.status.idle": "2024-03-25T00:33:09.066162Z",
          "shell.execute_reply.started": "2024-03-25T00:33:04.707286Z",
          "shell.execute_reply": "2024-03-25T00:33:09.065232Z"
        },
        "trusted": true,
        "id": "8ZK1II37Mj7-",
        "outputId": "4520f629-3c01-47ee-c74c-d3b4f48d9352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "\n\nQuestion:\nHow to export a notebook?\n\nAnswer:\nYou can save a notebook by clicking the \"Save\" button at the bottom of the notebook. You can also export notebooks as a .nb file by clicking the \"Save as\" button.\n\nNote that the \"Save as\" option will only work if you have a valid account. If you do not have a valid account, you can export notebooks as an HTML file by using the \"Export to HTML\" option in the \"File\" menu.\n\nIf you are using the JupyterHub notebook server, you can export notebooks as an HTML file using the \"Save as HTML\" button in the \"File\" menu.\n\nFinally, you can also export notebooks as an XML file by using the \"Export as XML\" button in the \"File\" menu."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "The result is not bad, especially compared to the model without fine-tuning. It's important to remember that we only fine-tuned this model using $500$ samples without any augmentation or advanced prompting. Therefore, there is ample room for improvement. Here are some tips to improve performance:\n",
        "\n",
        "- Increase number of samples used for finetuning\n",
        "- Experiment with advanced prompt engineering techniques.\n",
        "- Utilize a learning rate scheduler."
      ],
      "metadata": {
        "id": "J9KFxONMMj7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference\n",
        "* [Fine-tune Gemma models in Keras using LoRA](https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora)\n",
        "* [Parameter-efficient fine-tuning of GPT-2 with LoRA](https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/)\n",
        "* [Gemma - KerasNLP](https://keras.io/api/keras_nlp/models/gemma/)\n",
        "* [Python Questions from Stack Overflow](https://www.kaggle.com/datasets/stackoverflow/pythonquestions)"
      ],
      "metadata": {
        "id": "Fh83PWAcMj7-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pb29yjanP2dS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}